{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anavlajovic/ml-lab/blob/main/Pytorch%20mreza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vvopt6oHjMEK",
        "outputId": "eb24e16c-6366-47f4-edac-23d635efdcb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Accuracy: 0.09368191721132897\n",
            "1 Accuracy: 0.09368191721132897\n",
            "2 Accuracy: 0.09368191721132897\n",
            "3 Accuracy: 0.09368191721132897\n",
            "4 Accuracy: 0.09368191721132897\n",
            "5 Accuracy: 0.09368191721132897\n",
            "6 Accuracy: 0.09368191721132897\n",
            "7 Accuracy: 0.09368191721132897\n",
            "8 Accuracy: 0.09368191721132897\n",
            "9 Accuracy: 0.09368191721132897\n",
            "10 Accuracy: 0.09368191721132897\n",
            "11 Accuracy: 0.09368191721132897\n",
            "12 Accuracy: 0.09368191721132897\n",
            "13 Accuracy: 0.09368191721132897\n",
            "14 Accuracy: 0.09368191721132897\n",
            "15 Accuracy: 0.09368191721132897\n",
            "16 Accuracy: 0.09368191721132897\n",
            "17 Accuracy: 0.09368191721132897\n",
            "18 Accuracy: 0.09368191721132897\n",
            "19 Accuracy: 0.09368191721132897\n",
            "20 Accuracy: 0.09368191721132897\n",
            "21 Accuracy: 0.09368191721132897\n",
            "22 Accuracy: 0.09368191721132897\n",
            "23 Accuracy: 0.09368191721132897\n",
            "24 Accuracy: 0.09368191721132897\n",
            "25 Accuracy: 0.09368191721132897\n",
            "26 Accuracy: 0.09368191721132897\n",
            "27 Accuracy: 0.09368191721132897\n",
            "28 Accuracy: 0.09368191721132897\n",
            "29 Accuracy: 0.09368191721132897\n",
            "30 Accuracy: 0.09368191721132897\n",
            "31 Accuracy: 0.09368191721132897\n",
            "32 Accuracy: 0.09368191721132897\n",
            "33 Accuracy: 0.09368191721132897\n",
            "34 Accuracy: 0.09368191721132897\n",
            "35 Accuracy: 0.09368191721132897\n",
            "36 Accuracy: 0.09368191721132897\n",
            "37 Accuracy: 0.09368191721132897\n",
            "38 Accuracy: 0.09368191721132897\n",
            "39 Accuracy: 0.09368191721132897\n",
            "40 Accuracy: 0.09368191721132897\n",
            "41 Accuracy: 0.09368191721132897\n",
            "42 Accuracy: 0.09368191721132897\n",
            "43 Accuracy: 0.09368191721132897\n",
            "44 Accuracy: 0.09368191721132897\n",
            "45 Accuracy: 0.09368191721132897\n",
            "46 Accuracy: 0.09368191721132897\n",
            "47 Accuracy: 0.09368191721132897\n",
            "48 Accuracy: 0.09368191721132897\n",
            "49 Accuracy: 0.09368191721132897\n",
            "50 Accuracy: 0.09368191721132897\n",
            "51 Accuracy: 0.09368191721132897\n",
            "52 Accuracy: 0.09368191721132897\n",
            "53 Accuracy: 0.09368191721132897\n",
            "54 Accuracy: 0.09368191721132897\n",
            "55 Accuracy: 0.09368191721132897\n",
            "56 Accuracy: 0.09368191721132897\n",
            "57 Accuracy: 0.09368191721132897\n",
            "58 Accuracy: 0.09368191721132897\n",
            "59 Accuracy: 0.09368191721132897\n",
            "60 Accuracy: 0.09368191721132897\n",
            "61 Accuracy: 0.09368191721132897\n",
            "62 Accuracy: 0.09368191721132897\n",
            "63 Accuracy: 0.09368191721132897\n",
            "64 Accuracy: 0.09368191721132897\n",
            "65 Accuracy: 0.09368191721132897\n",
            "66 Accuracy: 0.09368191721132897\n",
            "67 Accuracy: 0.09368191721132897\n",
            "68 Accuracy: 0.09368191721132897\n",
            "69 Accuracy: 0.09368191721132897\n",
            "70 Accuracy: 0.09368191721132897\n",
            "71 Accuracy: 0.09368191721132897\n",
            "72 Accuracy: 0.09368191721132897\n",
            "73 Accuracy: 0.09368191721132897\n",
            "74 Accuracy: 0.09368191721132897\n",
            "75 Accuracy: 0.09368191721132897\n",
            "76 Accuracy: 0.09368191721132897\n",
            "77 Accuracy: 0.09368191721132897\n",
            "78 Accuracy: 0.09368191721132897\n",
            "79 Accuracy: 0.09368191721132897\n",
            "80 Accuracy: 0.09368191721132897\n",
            "81 Accuracy: 0.09368191721132897\n",
            "82 Accuracy: 0.09368191721132897\n",
            "83 Accuracy: 0.09368191721132897\n",
            "84 Accuracy: 0.09368191721132897\n",
            "85 Accuracy: 0.09368191721132897\n",
            "86 Accuracy: 0.09368191721132897\n",
            "87 Accuracy: 0.09368191721132897\n",
            "88 Accuracy: 0.09368191721132897\n",
            "89 Accuracy: 0.09368191721132897\n",
            "90 Accuracy: 0.09368191721132897\n",
            "91 Accuracy: 0.09368191721132897\n",
            "92 Accuracy: 0.09368191721132897\n",
            "93 Accuracy: 0.09368191721132897\n",
            "94 Accuracy: 0.09368191721132897\n",
            "95 Accuracy: 0.09368191721132897\n",
            "96 Accuracy: 0.09368191721132897\n",
            "97 Accuracy: 0.09368191721132897\n",
            "98 Accuracy: 0.09368191721132897\n",
            "99 Accuracy: 0.09368191721132897\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch as t\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import *\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "train, test = split( pd.read_csv(\"mnist_train.csv\").rename(columns={\"6\": \"label\"}), test_size=0.2)\n",
        "train = np.array(train).T\n",
        "#test = np.array(test).T\n",
        "\n",
        "X = t.from_numpy((train[1:] / 255).T).float()\n",
        "Y = t.from_numpy(train[0]).long()\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = Sequential( Linear(784, 150), ReLU(), Linear(150, 10), Softmax() )\n",
        "    def pred(self, x):\n",
        "        return t.argmax( self.net(x), dim=1)\n",
        "\n",
        "model=Model()\n",
        "\n",
        "def fit(loss_fn, opt):\n",
        "    ohy = F.one_hot(Y.to(t.int64), 10).float()\n",
        "    for i in range(100):\n",
        "        loss_fn(model.net(X), ohy).backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        print(i, \"Accuracy:\", t.sum(model.pred(X) == Y).item() / len(Y))\n",
        "\n",
        "fit(F.cross_entropy, t.optim.Adam(model.parameters(), lr=1e-3) )"
      ]
    }
  ]
}